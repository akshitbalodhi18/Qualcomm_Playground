{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a very basic code to test how playgrounds work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"IMAGINE_API_KEY\"]=os.getenv(\"IMAGINE_API_KEY\")\n",
    "os.environ[\"IMAGINE_API_ENDPOINT\"]=os.getenv(\"IMAGINE_ENDPOINT_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is how chatbot is being implemented in Qualcomm playgrounds. Basic chat completion chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain has a rich variety of delicious cheeses, and the \"best\" one is subjective, as it depends on personal taste preferences and the type of dish or occasion. Here are some of the most popular and iconic Spanish cheeses:\n",
      "\n",
      "1. **Manchego**: A firm, nutty, and slightly sweet cheese from the La Mancha region, often served as a table cheese or used in tapas.\n",
      "2. **Idiaz√°bal**: A semi-soft, creamy cheese from the Basque\n"
     ]
    }
   ],
   "source": [
    "from imagine import ChatMessage, ImagineClient\n",
    "\n",
    "client = ImagineClient()\n",
    "\n",
    "chat_response = client.chat(\n",
    "    messages=[ChatMessage(role=\"user\", content=\"What is the best Spanish cheese?\")],\n",
    "    model=\"Llama-3.1-8B\",\n",
    ")\n",
    "\n",
    "print(chat_response.first_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List of models available with playgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ModelType.EMBEDDING>: ['BAAI/bge-large-en-v1.5'],\n",
      " <ModelType.LLM>: ['DeepSeek-R1-Distill-Llama-70B',\n",
      "                   'Llama-3.1-8B',\n",
      "                   'Llama-3.3-70B',\n",
      "                   'Phi-4',\n",
      "                   'Qwen2.5-Coder-32B'],\n",
      " <ModelType.TEXT_TO_IMAGE>: ['stabilityai/sdxl-turbo']}\n",
      "['DeepSeek-R1-Distill-Llama-70B',\n",
      " 'Llama-3.1-8B',\n",
      " 'Llama-3.3-70B',\n",
      " 'Phi-4',\n",
      " 'Qwen2.5-Coder-32B']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from imagine import ImagineClient, ModelType\n",
    "\n",
    "\n",
    "client = ImagineClient()\n",
    "\n",
    "all_models = client.get_available_models_by_type()\n",
    "pprint(all_models)\n",
    "\n",
    "llm_models = client.get_available_models(model_type=ModelType.LLM)\n",
    "pprint(llm_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chat completion in Qualcomm Imagine Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " up to n terms\n",
      "def fibonacci(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while len(fib_sequence) < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence[:n]\n",
      "\n",
      "# Test the function\n",
      "print(fibonacci(10))  # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "\n",
      "# Time\n"
     ]
    }
   ],
   "source": [
    "from imagine import ImagineClient\n",
    "\n",
    "client = ImagineClient()\n",
    "\n",
    "completion_response = client.completion(\n",
    "    prompt=\"Write a Python function to get the fibonacci series\"\n",
    ")\n",
    "\n",
    "print(completion_response.first_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default embedding given in Qualcomm, the dimensity is around 1024. Looks good for an emmbedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "embedding_response = client.embeddings([\"What a beautiful day\", \"this is amazing\"])\n",
    "print(len(embedding_response.data[0].embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's see how Images work in imagine client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImagineAPITooManyRequestsException",
     "evalue": "Status: 429. Message: {\"message\":\"Too many requests. Invalid model/rate limits not configured for this model.\",\"status\":\"error\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImagineAPITooManyRequestsException\u001b[0m        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m images_response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mA cat sleeping on planet Mars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb64_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save image to file\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images_response\u001b[38;5;241m.\u001b[39mdata)):\n",
      "File \u001b[1;32md:\\TCS\\NextGen_Playground\\venv\\lib\\site-packages\\imagine\\client.py:827\u001b[0m, in \u001b[0;36mImagineClient.images_generate\u001b[1;34m(self, prompt, model, negative_prompt, seed, seed_increment, n, num_inference_steps, size, guidance_scale, cache_interval, response_format)\u001b[0m\n\u001b[0;32m    810\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_model_tti\n\u001b[0;32m    812\u001b[0m request_body \u001b[38;5;241m=\u001b[39m ImageRequest(\n\u001b[0;32m    813\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    814\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m     response_format\u001b[38;5;241m=\u001b[39mresponse_format,\n\u001b[0;32m    825\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump(exclude_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 827\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImagineException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo response received\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\TCS\\NextGen_Playground\\venv\\lib\\site-packages\\imagine\\client.py:227\u001b[0m, in \u001b[0;36mImagineClient._request\u001b[1;34m(self, method, path, json, attempt, data, files, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImagineException(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected exception (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImagineAPIException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[0;32m    230\u001b[0m         response,\n\u001b[0;32m    231\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    232\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32md:\\TCS\\NextGen_Playground\\venv\\lib\\site-packages\\imagine\\client.py:126\u001b[0m, in \u001b[0;36mImagineClient._check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_check_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: Response) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response_status_codes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     json_response: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[1;32md:\\TCS\\NextGen_Playground\\venv\\lib\\site-packages\\imagine\\client.py:104\u001b[0m, in \u001b[0;36mImagineClient._check_response_status_codes\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImagineAPIStatusException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[0;32m    100\u001b[0m         response,\n\u001b[0;32m    101\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    102\u001b[0m     )\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mTOO_MANY_REQUESTS:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ImagineAPITooManyRequestsException\u001b[38;5;241m.\u001b[39mfrom_response(\n\u001b[0;32m    105\u001b[0m         response,\n\u001b[0;32m    106\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_client_error(status_code):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstream:\n",
      "\u001b[1;31mImagineAPITooManyRequestsException\u001b[0m: Status: 429. Message: {\"message\":\"Too many requests. Invalid model/rate limits not configured for this model.\",\"status\":\"error\"}"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "client = ImagineClient(model=\"stabilityai/sdxl-turbo\")\n",
    "images_response = client.images_generate(\n",
    "    prompt=\"A cat sleeping on planet Mars\",\n",
    "    n=2,\n",
    "    negative_prompt=\"disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, b&w\",\n",
    "    response_format=\"b64_json\",\n",
    ")\n",
    "\n",
    "# Save image to file\n",
    "\n",
    "for i in range(len(images_response.data)):\n",
    "    with open(f\"MyImage_{i}.png\", \"wb\") as f:\n",
    "        f.write(base64.decodebytes(images_response.data[0].b64_json.encode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
